{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1060'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data Builder\n",
    "class build_data(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Loads and Prepares dataset for pytorch\n",
    "    WMAE weights are last two values in y\"\"\"\n",
    "    \n",
    "    def __init__(self, df, drop, split_size=0.33, rand=22391, batch=1, shuffle=True, pin=True, ts_only=True, wt=True):\n",
    "        self.wt = wt\n",
    "        self.rand = rand\n",
    "        self.split_size = split_size\n",
    "        self.batch = batch\n",
    "        self.shuffle = shuffle\n",
    "        self.pin = pin\n",
    "        self.ts = ts_only\n",
    "        \n",
    "        df = df.astype('float')\n",
    "        \n",
    "        ccols = [i for i in df.columns if 'Feature' in i]\n",
    "        self.keep = [i for i in ccols if i not in drop]\n",
    "        \n",
    "        self.x = df.iloc[:,26:147] # time steps\n",
    "        self.x2 = df.loc[:,self.keep] # other features\n",
    "        self.y = df.iloc[:,147:]\n",
    "               \n",
    "    def _na_fill(self,mode):\n",
    "        for i in self.x2.columns:\n",
    "            if i in mode:\n",
    "                self.x2[i] = self.x2[i].fillna(value=self.x2[i].mode()[0])\n",
    "            else:\n",
    "                self.x2[i] = self.x2[i].fillna(value=self.x2[i].median())\n",
    "                \n",
    "        self.x = self.x.interpolate(method='linear', axis=1)\n",
    "        self.x_fin = pd.concat([self.x2,self.x], axis=1)\n",
    "        \n",
    "    def _split(self):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.x_fin, self.y, test_size=self.split_size, random_state=self.rand)\n",
    "\n",
    "        # Seperate Features and TS\n",
    "        self.X_train_ts = X_train.iloc[:,23:147]\n",
    "        self.X_test_ts = X_test.iloc[:,23:147]\n",
    "\n",
    "        self.X_train_ft = X_train.iloc[:,:23]\n",
    "        self.X_test_ft = X_test.iloc[:,:23]\n",
    "\n",
    "        # Get Weights for MAE\n",
    "        # Weights also included in loader, be sure to index when running\n",
    "        self.test_wt, self.train_wt = np.asarray(y_test.iloc[:,-2:]), np.asarray(y_train.iloc[:,-2:])\n",
    "        self.y_test, self.y_train = np.asarray(y_test.iloc[:,:-2]), np.asarray(y_train.iloc[:,:-2])\n",
    "        \n",
    "    def _scale(self,stsc,lab,dev=True):\n",
    "        ctrans =  ColumnTransformer(\n",
    "                    [('scale_all', StandardScaler(), stsc),\n",
    "                     ('cats', OneHotEncoder(categories='auto'), lab)])\n",
    "        \n",
    "        xtsc = StandardScaler()\n",
    "        ytsc = StandardScaler()\n",
    "        wtsc = StandardScaler(with_mean=False)\n",
    "        \n",
    "        self.X_train_ft = ctrans.fit_transform(self.X_train_ft)\n",
    "        self.X_test_ft = ctrans.transform(self.X_test_ft)\n",
    "        self.X_train_ts = xtsc.fit_transform(self.X_train_ts)\n",
    "        self.X_test_ts = xtsc.transform(self.X_test_ts)\n",
    "       \n",
    "        if self.ts:\n",
    "            self.x_train = self.X_train_ts\n",
    "            self.x_test = self.X_test_ts\n",
    "        else:\n",
    "            self.x_train = np.concatenate([self.X_train_ft, self.X_train_ts], axis=1)\n",
    "            self.x_test = np.concatenate([self.X_test_ft, self.X_test_ts], axis=1)\n",
    "        \n",
    "        self.train_wt = wtsc.fit_transform(self.train_wt)\n",
    "        self.test_wt = wtsc.transform(self.test_wt)\n",
    "        \n",
    "        self.y_train_sc = ytsc.fit_transform(self.y_train)\n",
    "        self.y_test_sc = ytsc.transform(self.y_test)\n",
    "        \n",
    "        if self.wt:\n",
    "            self.y_train = np.concatenate([self.y_train,self.train_wt],axis=1)\n",
    "            self.y_test = np.concatenate([self.y_test,self.test_wt],axis=1)\n",
    "        \n",
    "        self.xtrans = xtsc\n",
    "        self.ytrans = ytsc\n",
    "        self.ftrans = ctrans\n",
    "        \n",
    "    def fit(self, mode, stsc, lab):\n",
    "        self._na_fill(mode)\n",
    "        self._split()\n",
    "        self._scale(stsc,lab)\n",
    "        self.mode = mode\n",
    "        \n",
    "        torch_x_train, torch_y_train = torch.from_numpy(self.x_train).float(), torch.from_numpy(self.y_train_sc).float()\n",
    "        torch_x_test, torch_y_test = torch.from_numpy(self.x_test).float(), torch.from_numpy(self.y_test_sc).float()\n",
    "        \n",
    "        train = data_utils.TensorDataset(torch_x_train, torch_y_train)\n",
    "        test = data_utils.TensorDataset(torch_x_test, torch_y_test)\n",
    "        \n",
    "        train_loader = data_utils.DataLoader(train, batch_size=self.batch, shuffle=self.shuffle, pin_memory=self.pin)\n",
    "        test_loader = data_utils.DataLoader(test, batch_size=self.batch, shuffle=self.shuffle, pin_memory=self.pin)\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    def fit_sub(self, sub_df, ft=False):\n",
    "        sub_df = sub_df.astype('float')\n",
    "        sub_x = sub_df.iloc[:,26:147] # time steps\n",
    "        sub_x2 = sub_df.loc[:,self.keep] # other features\n",
    "        \n",
    "        # Fill NA\n",
    "        for i in sub_x2.columns:\n",
    "            if i in self.mode:\n",
    "                sub_x2[i] = sub_x2[i].fillna(value=sub_x2[i].mode()[0])\n",
    "            else:\n",
    "                sub_x2[i] = sub_x2[i].fillna(value=sub_x2[i].median())\n",
    "                \n",
    "        sub_x = sub_x.interpolate(method='linear', axis=1)\n",
    "        \n",
    "        # Scale\n",
    "        sub_x = self.xtrans.transform(sub_x)\n",
    "        sub_x2 = self.ftrans.transform(sub_x2)\n",
    "        if ft:\n",
    "            sub = np.concatenate([sub_x2, sub_x], axis=1)\n",
    "        else:\n",
    "            sub = sub_x\n",
    "        \n",
    "        # Make loader\n",
    "        sub = torch.from_numpy(sub).float()\n",
    "        sub_ds = data_utils.TensorDataset(sub)\n",
    "        sub_loader = data_utils.DataLoader(sub_ds, batch_size=self.batch, shuffle=False, pin_memory=self.pin)\n",
    "        \n",
    "        return sub_loader\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.train_wt, self.test_wt\n",
    "    \n",
    "    def reverse_trans(self, x=False, y=False):\n",
    "        if x is not False and y is not False:\n",
    "            return self.xtrans.inverse_transform(x), self.ytrans.inverse_transform(y)\n",
    "        elif x is not False:\n",
    "            return self.xtrans.inverse_transform(x)\n",
    "        elif y is not False:\n",
    "            return self.ytrans.inverse_transform(y)\n",
    "        \n",
    "    def make_sub(sub, fn, path=r\"C:\\Users\\rlagr\\fin\\winton\\data\\\\\"):\n",
    "        sub = self.ytrans.inverse_transform(sub)\n",
    "        sub = sub.reshape(-1,1)\n",
    "\n",
    "        win = [i for i in range(1,120001)]\n",
    "        step = [i for i in range(1,63)]\n",
    "        rnames = [None] * sub.shape[0]\n",
    "\n",
    "        ind = 0\n",
    "\n",
    "        for i in win:\n",
    "            for k in step:\n",
    "                name = str(i) + '_' + str(k)\n",
    "                rnames[ind] = name\n",
    "                ind += 1\n",
    "\n",
    "        s = pd.DataFrame(rnames)\n",
    "        s.columns = ['Id']\n",
    "        s['Predicted'] = sub\n",
    "        path = path + fn\n",
    "\n",
    "        s.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Loss function\n",
    "class wmae_loss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(wmae_loss,self).__init__()\n",
    "        \n",
    "    def _wmae(self,pred, true, wts):\n",
    "        \"\"\"Second weight for last 2 preds, first for rest\"\"\"\n",
    "        n = true.shape[0] * true.shape[1]\n",
    "        intra = torch.sum(wts[0] * torch.abs(true[:,:-2] - pred[:-2]))\n",
    "        daily = torch.sum(wts[1] * torch.abs(true[:,-2:] - pred[-2:]))\n",
    "        return (intra + daily) / n\n",
    "        \n",
    "    def forward(self, pred, true, wt):\n",
    "        return self._wmae(pred, true, wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this custom WMAE with the weights provided doesn't seem to have any noticable effect.  The net is still converging at predicting zero across the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mod_mse(torch.nn.Module):\n",
    "    def __init__(self, l1=1, l2=1):\n",
    "        super(mod_mse, self).__init__()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        \n",
    "    def _mod(self, pred, true):\n",
    "        pred_div = torch.clamp(pred, min=0.00001)\n",
    "        return torch.mean((self.l1 * torch.abs(true/pred_div)) + (self.l2 * (true - pred)**2))\n",
    "    \n",
    "    def forward(self, pred, true):\n",
    "        return self._mod(pred,true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss function was made in an attempt to punish inaccurate predictions that were close to zero.  Another variation where the two terms were multiplied was also tried but both had the effect of just shifting the predictions up to a small positive number (like 1 or 2) but didn't encourage making them with any greater magnitude.  For now it's probably best to just stick to standard MSE but this loss function may be helpful later when a different way of encouraging higher magnitude predictions can be found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dat, act=False, sub=False):\n",
    "    model.eval()\n",
    "    res = []\n",
    "    y_out = []\n",
    "    if sub:\n",
    "        for i, x in enumerate(dat):\n",
    "            x = x[0].view(-1,1,121)\n",
    "            x = x.cuda()\n",
    "            out = model(x)\n",
    "            out = out.cpu().data.numpy()\n",
    "            res.append(out)\n",
    "            \n",
    "    else:\n",
    "        for i, (x,y) in enumerate(dat):\n",
    "            true = y[:,:-2].cuda()\n",
    "            wts = torch.flatten(y[:,-2:]).cuda()\n",
    "            x = x.view(-1,1,121)\n",
    "            x = x.cuda()\n",
    "            out = model(x)\n",
    "            out = out.cpu().data.numpy()\n",
    "            res.append(out)\n",
    "\n",
    "            if act:\n",
    "                true = torch.flatten(true)\n",
    "                true = true.cpu().data.numpy()\n",
    "                y_out.append(true)\n",
    "    \n",
    "    res = np.array(res)\n",
    "    if act:\n",
    "        y_out = np.array(y_out)\n",
    "        return res, y_out\n",
    "    else:\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\rlagr\\fin\\winton\\data\\train.csv\"\n",
    "#path = r\"C:\\Users\\RemyLagrois\\!pytorch\\data\\train.csv\"\n",
    "exclude = ['Feature_1', 'Feature_10']\n",
    "mode = ['Feature_9', 'Feature_13', 'Feature_16', 'Feature_20']\n",
    "cats = ['Feature_5', 'Feature_13', 'Feature_16', 'Feature_20']\n",
    "scale = ['Feature_2', 'Feature_3', 'Feature_4', 'Feature_6', 'Feature_8', 'Feature_11', 'Feature_12', 'Feature_14', 'Feature_17', 'Feature_18',\n",
    "         'Feature_19', 'Feature_21', 'Feature_22', 'Feature_23', 'Feature_24', 'Feature_25', 'Feature_7', 'Feature_9', 'Feature_15']\n",
    "\n",
    "# Dev Data\n",
    "df = pd.read_csv(path)\n",
    "data = build_data(df, exclude, wt=False)\n",
    "\n",
    "train_loader, test_loader = data.fit(mode, scale, cats)\n",
    "\n",
    "# Submission Data\n",
    "#sub_set = pd.read_csv(r\"C:\\Users\\rlagr\\fin\\winton\\data\\test_2.csv\")\n",
    "#sub_load = data.fit_sub(sub_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen previously none of the architectures jumped out as being especially good and had a tendancy to train towards zero.  Here I have defined the custom loss function; it's a WMAE where the intra-day minute time steps get one weight and the final two full day returns get another.  I will define a 1d CNN similar to the one in Keras and see if this loss function leads to any better fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1d(nn.Module):\n",
    "    \n",
    "    def __init__(self, kernels, filters, hidden):\n",
    "        super(CNN_1d, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential()\n",
    "        self.reg = nn.Sequential()\n",
    "        lin_in = 60 * filters[-1]\n",
    "        gain = nn.init.calculate_gain('leaky_relu', 0.1)\n",
    "        # Create Conv layers\n",
    "        for i, k in enumerate(kernels):\n",
    "            pad = int((k - 1) / 2)\n",
    "            if i == 0:\n",
    "                conv = nn.Conv1d(1, filters[i], k, 1, padding=pad)\n",
    "            else:\n",
    "                conv = nn.Conv1d(filters[i-1], filters[i], k, 1, pad)\n",
    "            nn.init.xavier_uniform_(conv.weight,gain=gain)\n",
    "            self.cnn.add_module('conv-' + str(i + 1), conv)\n",
    "            self.cnn.add_module('lrel' + str(i + 1), nn.LeakyReLU(negative_slope=.1))\n",
    "            self.cnn.add_module('bn-'+ str(i +1), nn.BatchNorm1d(filters[i]))\n",
    "        self.cnn.add_module('mpool', nn.MaxPool1d(2)) # final max pool\n",
    "        \n",
    "        # Add input layer to feed forward section\n",
    "        lin = nn.Linear(lin_in, hidden[0])\n",
    "        nn.init.uniform_(lin.weight,a=-1.01, b=1.01)\n",
    "        self.reg.add_module('FF-in', lin)\n",
    "        self.reg.add_module('in-lrel', nn.LeakyReLU(negative_slope=.1))\n",
    "        \n",
    "        for i, k in enumerate(hidden):\n",
    "            if i < (len(hidden) - 1):\n",
    "                hid = nn.Linear(k, hidden[i + 1])\n",
    "                if i == (len(hidden) - 2):\n",
    "                    nn.init.normal_(hid.weight,std=1)\n",
    "                else:\n",
    "                    nn.init.uniform_(hid.weight,a=-1.01,b=1.01)\n",
    "                self.reg.add_module('hid-' + str(i+1), hid)\n",
    "                self.reg.add_module('h-lrel-' + str(i+1), nn.LeakyReLU(negative_slope=.1))\n",
    "                #self.cnn.add_module('rbn-'+ str(i +1), nn.BatchNorm1d(hidden))\n",
    "                \n",
    "            elif i == (len(hidden) - 1):\n",
    "                self.reg.add_module('out', nn.Linear(k, 62))\n",
    "                nn.init.normal_(hid.weight,std=.5)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = torch.flatten(x)\n",
    "        x = self.reg(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Initialization is starting to look like the key to getting predictions that are not close to zero.  The net is very sensitive to the starting values though with distributions containing values greater than +/- 1 producing predictions on the order of 10^5 or greater.  Using Xavier normal with the proper gain for LReLU with neg slope = 0.1 on the convolutions, Uniform from +/- 1.01 on the hidden layers, and normal with mean = 0 and std = 1.1 on the output is producing predictions about in the range of +/- 10-20 after 100 epochs.  This method seems to at least be on the right track; avoiding the zero prediction valley by converging on the correct predictions from above and below is looking like the best method.  Once the starting conditions have been tuned a bit to not require so many epochs it may be a good idea to re-implement the modified MSE to prevent overshooting correct predictions.  The training appears to mostly be modifying weights close to the output, the last hidden layer (hid-5) becomes more normally distributed with weights ranging from +/- 4 while the earlier hidden layers stay closer to uniform with ranges about the same as their starting conditions (though at a lower frequency).  The final linear layer stays normally distributed but with very small weights ranging from ~ +/- 0.02.  \n",
    "\n",
    "Running Xavier Uniform as before, uniform +/- 0.9 on the hidden layers, and normal mean = 0 with std = 0.85 on the final layer produced predictions in the correct range after 70 epochs.  However the loss spiked a lot which would seem to indicate an overly aggressive learning rate.  I suspect the submission predictions won't score as well as straight zero guesses since it doesn't appear to match up extremely well.  The patterns in the weights from the previous experiment held.  For this round the uniform weights will be increased back up to 1.01 but the std for the final layer will be made much smaller.  Finally layer 5 will be switched to a normal distribution with an std of ~1.  \n",
    "\n",
    "Xavier, uniform +/- 1.01, normal on the last hidden layer with std=1, and normal on the output with std=0.1 seems to be right about where we want to be. THe last hidden layer weights are getting smaller than they were before though.  It's not obvious if this is a good or bad thing. The eval data isn't keeping up as well and after ~40 epochs it's going past where we'd like.  Tuning other hyper parameters is likely the best next step and using the modified MSE function may also help keep predictions where we want them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_1d(\n",
      "  (cnn): Sequential(\n",
      "    (conv-1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (lrel1): LeakyReLU(negative_slope=0.1)\n",
      "    (bn-1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv-2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (lrel2): LeakyReLU(negative_slope=0.1)\n",
      "    (bn-2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv-3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (lrel3): LeakyReLU(negative_slope=0.1)\n",
      "    (bn-3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv-4): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (lrel4): LeakyReLU(negative_slope=0.1)\n",
      "    (bn-4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv-5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (lrel5): LeakyReLU(negative_slope=0.1)\n",
      "    (bn-5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv-6): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (lrel6): LeakyReLU(negative_slope=0.1)\n",
      "    (bn-6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (mpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (reg): Sequential(\n",
      "    (FF-in): Linear(in_features=1920, out_features=1000, bias=True)\n",
      "    (in-lrel): LeakyReLU(negative_slope=0.1)\n",
      "    (hid-1): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (h-lrel-1): LeakyReLU(negative_slope=0.1)\n",
      "    (out): Linear(in_features=1000, out_features=62, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "kernels = [3,3,3,3,3,3]\n",
    "filters = [32,32,32,32,32,32]\n",
    "hidden = [1000,1000]\n",
    "\n",
    "cnn1 = CNN_1d(kernels, filters, hidden)\n",
    "cnn1 = cnn1.cuda()\n",
    "\n",
    "#loss_fn = mod_mse()\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch.optim.RMSprop(cnn1.parameters(), lr=.0001)\n",
    "\n",
    "print(cnn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch  1\n",
      "\titeration 0 -- 22307.732421875\n",
      "\titeration 1000 -- 5942.81836461878\n",
      "\titeration 2000 -- 4000.7370102180357\n",
      "\titeration 3000 -- 2914.261905338398\n",
      "\titeration 4000 -- 2240.911444085504\n",
      "\titeration 5000 -- 1801.8272802804474\n",
      "\titeration 6000 -- 1502.9595295881315\n",
      "\titeration 7000 -- 1288.6772161606098\n",
      "\titeration 8000 -- 1127.8462117826707\n",
      "\titeration 9000 -- 1002.7007837377678\n",
      "\titeration 10000 -- 902.5510231791294\n",
      "\titeration 11000 -- 820.6255055036439\n",
      "\titeration 12000 -- 752.3581342694306\n",
      "\titeration 13000 -- 694.5750457570635\n",
      "\titeration 14000 -- 645.0487647249623\n",
      "\titeration 15000 -- 602.112188226644\n",
      "\titeration 16000 -- 564.5704701949011\n",
      "\titeration 17000 -- 531.4335254714755\n",
      "\titeration 18000 -- 501.9627177569926\n",
      "\titeration 19000 -- 475.6003587608924\n",
      "\titeration 20000 -- 451.8735568100636\n",
      "\titeration 21000 -- 430.40625596622317\n",
      "\titeration 22000 -- 410.8864902908977\n",
      "\titeration 23000 -- 393.07924592418726\n",
      "\titeration 24000 -- 376.74144139284573\n",
      "\titeration 25000 -- 361.7120747196521\n",
      "\titeration 26000 -- 347.8474199638025\n",
      "Epoch 1/15, Training Loss: 337.505, Testing Loss: 1.258\n",
      "starting epoch  2\n",
      "\titeration 0 -- 0.32622918486595154\n",
      "\titeration 1000 -- 0.9955921306402296\n",
      "\titeration 2000 -- 0.942355981965115\n",
      "\titeration 3000 -- 0.9696150078701699\n",
      "\titeration 4000 -- 1.038670223479091\n",
      "\titeration 5000 -- 1.1090655633745803\n",
      "\titeration 6000 -- 1.0970251595819023\n",
      "\titeration 7000 -- 1.0740112324942734\n",
      "\titeration 8000 -- 1.0871068838298872\n",
      "\titeration 9000 -- 1.0749388240950728\n",
      "\titeration 10000 -- 1.0698280739011767\n",
      "\titeration 11000 -- 1.064339056947492\n",
      "\titeration 12000 -- 1.0554652269457816\n",
      "\titeration 13000 -- 1.0563574678295862\n",
      "\titeration 14000 -- 1.0486304177912285\n",
      "\titeration 15000 -- 1.0513158243438334\n",
      "\titeration 16000 -- 1.0556588850762472\n",
      "\titeration 17000 -- 1.0530422195692717\n",
      "\titeration 18000 -- 1.047928444149643\n",
      "\titeration 19000 -- 1.045506497744574\n",
      "\titeration 20000 -- 1.041114135947948\n",
      "\titeration 21000 -- 1.0355445038668094\n",
      "\titeration 22000 -- 1.0349416612850197\n",
      "\titeration 23000 -- 1.0333635083736472\n",
      "\titeration 24000 -- 1.0265942732950395\n",
      "\titeration 25000 -- 1.0258584016839642\n",
      "\titeration 26000 -- 1.0283018342466765\n",
      "Epoch 2/15, Training Loss: 1.033, Testing Loss: 1.427\n",
      "starting epoch  3\n",
      "\titeration 0 -- 0.468572199344635\n",
      "\titeration 1000 -- 1.0498908100562259\n",
      "\titeration 2000 -- 0.9856783601733251\n",
      "\titeration 3000 -- 0.9974688970083815\n",
      "\titeration 4000 -- 0.9921537564312762\n",
      "\titeration 5000 -- 1.0156326829520042\n",
      "\titeration 6000 -- 1.0237368208062418\n",
      "\titeration 7000 -- 0.9984657433609774\n",
      "\titeration 8000 -- 1.014052588808078\n",
      "\titeration 9000 -- 1.0126495764055556\n",
      "\titeration 10000 -- 1.011755792362249\n",
      "\titeration 11000 -- 1.0137104177111826\n",
      "\titeration 12000 -- 1.0081075353125921\n",
      "\titeration 13000 -- 1.0110960483282252\n",
      "\titeration 14000 -- 1.0313014295895384\n",
      "\titeration 15000 -- 1.0298004012218884\n",
      "\titeration 16000 -- 1.019734216002917\n",
      "\titeration 17000 -- 1.021218637855016\n",
      "\titeration 18000 -- 1.032954889346794\n",
      "\titeration 19000 -- 1.0296956381378863\n",
      "\titeration 20000 -- 1.0281080822737174\n",
      "\titeration 21000 -- 1.0221921606125481\n",
      "\titeration 22000 -- 1.0190776007580218\n",
      "\titeration 23000 -- 1.0188794578533493\n",
      "\titeration 24000 -- 1.0128928066950844\n",
      "\titeration 25000 -- 1.0198548591887682\n",
      "\titeration 26000 -- 1.02664092594398\n",
      "Epoch 3/15, Training Loss: 1.023, Testing Loss: 1.289\n",
      "starting epoch  4\n",
      "\titeration 0 -- 2.4678714275360107\n",
      "\titeration 1000 -- 0.9747004059481097\n",
      "\titeration 2000 -- 0.9969515537527548\n",
      "\titeration 3000 -- 1.0020539836293307\n",
      "\titeration 4000 -- 0.9729683575297454\n",
      "\titeration 5000 -- 0.9453828838865975\n",
      "\titeration 6000 -- 0.9394856613008946\n",
      "\titeration 7000 -- 0.9252361106956206\n",
      "\titeration 8000 -- 0.9285770284901029\n",
      "\titeration 9000 -- 0.9385754186047516\n",
      "\titeration 10000 -- 0.9579003500812587\n",
      "\titeration 11000 -- 0.9681018612797669\n",
      "\titeration 12000 -- 0.9683431803398831\n",
      "\titeration 13000 -- 0.9811036504749786\n",
      "\titeration 14000 -- 0.9774099848907726\n",
      "\titeration 15000 -- 0.9727144600202309\n",
      "\titeration 16000 -- 0.9722863320419788\n",
      "\titeration 17000 -- 0.9843720437429767\n",
      "\titeration 18000 -- 0.9986549372676197\n",
      "\titeration 19000 -- 1.0019788388700048\n",
      "\titeration 20000 -- 0.9977344688385521\n",
      "\titeration 21000 -- 1.0019843390681626\n",
      "\titeration 22000 -- 1.015665346139562\n",
      "\titeration 23000 -- 1.0226166509154209\n",
      "\titeration 24000 -- 1.022930571971765\n",
      "\titeration 25000 -- 1.0171779300361898\n",
      "\titeration 26000 -- 1.010417991426232\n",
      "Epoch 4/15, Training Loss: 1.011, Testing Loss: 1.224\n",
      "starting epoch  5\n",
      "\titeration 0 -- 0.22035612165927887\n",
      "\titeration 1000 -- 0.9688965890158366\n",
      "\titeration 2000 -- 1.0185720023793587\n",
      "\titeration 3000 -- 0.9934848580547906\n",
      "\titeration 4000 -- 0.9961709681580854\n",
      "\titeration 5000 -- 0.9801371788832872\n",
      "\titeration 6000 -- 0.9988385469396176\n",
      "\titeration 7000 -- 0.9926078946226823\n",
      "\titeration 8000 -- 0.9734652374342965\n",
      "\titeration 9000 -- 0.9672433443633603\n",
      "\titeration 10000 -- 0.9617082600409824\n",
      "\titeration 11000 -- 0.9632190421730572\n",
      "\titeration 12000 -- 0.9541073254343818\n",
      "\titeration 13000 -- 0.9697335923470097\n",
      "\titeration 14000 -- 0.9701144933628988\n",
      "\titeration 15000 -- 0.9734881268125695\n",
      "\titeration 16000 -- 0.9689312499526623\n",
      "\titeration 17000 -- 0.9699307029630686\n",
      "\titeration 18000 -- 0.9663862264447395\n",
      "\titeration 19000 -- 0.9720335657158454\n",
      "\titeration 20000 -- 0.9646418413636157\n",
      "\titeration 21000 -- 0.9636221025998973\n",
      "\titeration 22000 -- 0.9639131807145647\n",
      "\titeration 23000 -- 0.9723012511702697\n",
      "\titeration 24000 -- 0.9782256672431549\n",
      "\titeration 25000 -- 0.9789308795620429\n",
      "\titeration 26000 -- 0.9784505197012612\n",
      "Epoch 5/15, Training Loss: 0.978, Testing Loss: 1.396\n",
      "starting epoch  6\n",
      "\titeration 0 -- 0.5789657235145569\n",
      "\titeration 1000 -- 1.1273638194428754\n",
      "\titeration 2000 -- 0.958429358475588\n",
      "\titeration 3000 -- 0.9513943323987185\n",
      "\titeration 4000 -- 0.9273754852386068\n",
      "\titeration 5000 -- 0.9103080538632056\n",
      "\titeration 6000 -- 0.9270732349645824\n",
      "\titeration 7000 -- 0.926911273725765\n",
      "\titeration 8000 -- 0.9630030787224806\n",
      "\titeration 9000 -- 0.9650241860040893\n",
      "\titeration 10000 -- 0.9601849747520715\n",
      "\titeration 11000 -- 0.9654668967019702\n",
      "\titeration 12000 -- 0.9573351758972317\n",
      "\titeration 13000 -- 0.9594847258939277\n",
      "\titeration 14000 -- 0.9487102037716361\n",
      "\titeration 15000 -- 0.9435297294959583\n",
      "\titeration 16000 -- 0.9468041178990083\n",
      "\titeration 17000 -- 0.9411841145568756\n",
      "\titeration 18000 -- 0.938940874308399\n",
      "\titeration 19000 -- 0.9456920187282557\n",
      "\titeration 20000 -- 0.9492379950513963\n",
      "\titeration 21000 -- 0.9456957810385915\n",
      "\titeration 22000 -- 0.9459015021628846\n",
      "\titeration 23000 -- 0.9423470925177747\n",
      "\titeration 24000 -- 0.9399745478317489\n",
      "\titeration 25000 -- 0.9429724158791897\n",
      "\titeration 26000 -- 0.9465468030459533\n",
      "Epoch 6/15, Training Loss: 0.944, Testing Loss: 1.321\n",
      "starting epoch  7\n",
      "\titeration 0 -- 0.8201664686203003\n",
      "\titeration 1000 -- 0.8306591704945554\n",
      "\titeration 2000 -- 0.9291528994324564\n",
      "\titeration 3000 -- 0.9064365328270638\n",
      "\titeration 4000 -- 0.8879957606476401\n",
      "\titeration 5000 -- 0.8834436666983958\n",
      "\titeration 6000 -- 0.8872277922037517\n",
      "\titeration 7000 -- 0.8859775145031983\n",
      "\titeration 8000 -- 0.8888171356587619\n",
      "\titeration 9000 -- 0.8985665208659963\n",
      "\titeration 10000 -- 0.906765413377164\n",
      "\titeration 11000 -- 0.9023480149601107\n",
      "\titeration 12000 -- 0.8934845974833846\n",
      "\titeration 13000 -- 0.8936012029590806\n",
      "\titeration 14000 -- 0.888499230244323\n",
      "\titeration 15000 -- 0.893497060020146\n",
      "\titeration 16000 -- 0.9014195494402546\n",
      "\titeration 17000 -- 0.9067166115030599\n",
      "\titeration 18000 -- 0.9033781216309571\n",
      "\titeration 19000 -- 0.8988868831824748\n",
      "\titeration 20000 -- 0.9255497296458617\n",
      "\titeration 21000 -- 0.9258550283331476\n",
      "\titeration 22000 -- 0.9218353129596802\n",
      "\titeration 23000 -- 0.9217448564332784\n",
      "\titeration 24000 -- 0.916127083553517\n",
      "\titeration 25000 -- 0.9147424432439916\n",
      "\titeration 26000 -- 0.9121130823195087\n",
      "Epoch 7/15, Training Loss: 0.916, Testing Loss: 1.247\n",
      "starting epoch  8\n",
      "\titeration 0 -- 0.804355800151825\n",
      "\titeration 1000 -- 0.863268246086171\n",
      "\titeration 2000 -- 0.9061470983320962\n",
      "\titeration 3000 -- 0.9159283109033473\n",
      "\titeration 4000 -- 0.8751630515748878\n",
      "\titeration 5000 -- 0.8934246260040379\n",
      "\titeration 6000 -- 0.9111826455877363\n",
      "\titeration 7000 -- 0.9063181143147354\n",
      "\titeration 8000 -- 0.8949708667770121\n",
      "\titeration 9000 -- 0.8880968380390599\n",
      "\titeration 10000 -- 0.8814964241738112\n",
      "\titeration 11000 -- 0.8700058688500634\n",
      "\titeration 12000 -- 0.8705712713373508\n",
      "\titeration 13000 -- 0.8632479639757279\n",
      "\titeration 14000 -- 0.8850814664772177\n",
      "\titeration 15000 -- 0.8872217380799764\n",
      "\titeration 16000 -- 0.8944373527876662\n",
      "\titeration 17000 -- 0.8930803076243309\n"
     ]
    }
   ],
   "source": [
    "#Training the CNN\n",
    "num_epochs = 15\n",
    "\n",
    "#Define the lists to store the results of loss and accuracy\n",
    "\n",
    "losses = {'train_loss' : [None]*num_epochs , 'eval_loss' : [None]*num_epochs}\n",
    "\n",
    "#Training\n",
    "for epoch in range(num_epochs): \n",
    "    \n",
    "    print('starting epoch ', str(epoch + 1))\n",
    "    iterations = 0\n",
    "    iter_loss = 0.0\n",
    "    \n",
    "    cnn1.train()\n",
    "    for i, (x,y) in enumerate(train_loader):\n",
    "\n",
    "        true = y.cuda(non_blocking=True)\n",
    "        true = torch.flatten(true)\n",
    "        x = x.view(-1,1,121)\n",
    "        x = x.cuda(non_blocking=True)\n",
    "        opt.zero_grad()\n",
    "        out = cnn1(x)\n",
    "        loss = loss_fn(out, true)\n",
    "    \n",
    "        iter_loss += loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        iterations += 1\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print('\\titeration', str(i), '--', str(iter_loss/iterations))\n",
    "        \n",
    "    losses['train_loss'][epoch] = iter_loss / iterations\n",
    "\n",
    "    # Test\n",
    "    cnn1.eval()\n",
    "    ev_loss = 0.0\n",
    "    iterations = 0\n",
    "    \n",
    "    for i, (x,y) in enumerate(test_loader):\n",
    "        \n",
    "        true = y.cuda(non_blocking=True)\n",
    "        true = torch.flatten(true)\n",
    "        x = x.view(-1,1,121)\n",
    "        x = x.cuda(non_blocking=True)\n",
    "        out = cnn1(x)\n",
    "        loss = loss_fn(out, true)\n",
    "        ev_loss += loss.item()\n",
    "        \n",
    "        iterations += 1\n",
    "    \n",
    "    losses['eval_loss'][epoch] = ev_loss / iterations\n",
    "    \n",
    "    print ('Epoch {}/{}, Training Loss: {:.3f}, Testing Loss: {:.3f}'\n",
    "           .format(epoch+1, num_epochs, losses['train_loss'][epoch], losses['eval_loss'][epoch]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_train = losses['train_loss']\n",
    "tot_eval = losses['eval_loss'],\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(losses['train_loss'], label='Training Loss')\n",
    "plt.plot(losses['eval_loss'], label='Testing Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,7))\n",
    "#test, test2 = predict(cnn1, train_loader, True)\n",
    "plt.plot(test[25], label='Pred')\n",
    "plt.plot(test2[25], label='True')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_1_w = cnn1.state_dict()['reg.FF-in.weight'].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.hist(hid_1_w[15:20], stacked=True)\n",
    "plt.title('First Hidden Input Layer Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_hid3_w = cnn1.state_dict()['reg.hid-1.weight'].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.hist(reg_hid3_w[15:20], stacked=True)\n",
    "plt.title('Regression Hidden Layer Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_hid5_w = cnn1.state_dict()['reg.hid-4.weight'].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.hist(reg_hid5_w[15:20], stacked=True)\n",
    "plt.title('Regression Last Hid Layer Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_out_w = cnn1.state_dict()['reg.out.weight'].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.hist(reg_out_w[15:20], stacked=True)\n",
    "plt.title('Regression Output Layer Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn_3 = cnn1.state_dict()['cnn.conv-1.weight'].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sub(sub, fn):\n",
    "    sub = data.reverse_trans(y=sub)\n",
    "    sub = sub.reshape(-1,1)\n",
    "\n",
    "    win = [i for i in range(1,120001)]\n",
    "    step = [i for i in range(1,63)]\n",
    "    rnames = [None] * sub.shape[0]\n",
    "\n",
    "    ind = 0\n",
    "\n",
    "    for i in win:\n",
    "        for k in step:\n",
    "            name = str(i) + '_' + str(k)\n",
    "            rnames[ind] = name\n",
    "            ind += 1\n",
    "\n",
    "    s = pd.DataFrame(rnames)\n",
    "    s.columns = ['Id']\n",
    "    s['Predicted'] = sub\n",
    "    path = r\"C:\\Users\\rlagr\\fin\\winton\\data\\\\\"\n",
    "    path = path + fn\n",
    "    \n",
    "    s.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "for i in sub_load:\n",
    "    if it == 1:\n",
    "        break\n",
    "    \n",
    "    print(i[0].view(-1,1))\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_f = predict(cnn1, sub_load, sub=True)\n",
    "\n",
    "make_sub(sub_f, fn=\"pytorch_cnn1_winit.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
